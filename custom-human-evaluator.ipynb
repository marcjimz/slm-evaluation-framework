{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ad5da8",
   "metadata": {},
   "source": [
    "# Creating Human Classifier with LLMs - A Python SDK Experience\n",
    "\n",
    "This notebook will conduct a deep-dive onto a custom-evaluator to evaluate human scoring. This might be needed as part of system operations to properly evaluate benchmark and test datasets, as a correlated metric that can best emulate the evaluation criteria by the business and evaluations conducted by humans must be measured. This is different than simply evaluating physical structure and nuance of english language - the human element also includes other elements that are not structurally consistent in evaluation such as bias and variability.\n",
    "\n",
    "# ‚ö†Ô∏è üß™ CAUTION: This is not meant for production workloads!\n",
    "\n",
    "Please note, that this document does not **recommend** using LLM's in replacement of human evaluations. Multiple studies have been conducted across standardized datasets, and as of June 2024 the research suggests LLMs are not ready for full on evaluation in lieu of humans. However, we do proceed with a framework below to anticipate advances and explore the possibility using known techniques to us.\n",
    "\n",
    "To read more on the publication, you can refer to the study (here)[https://arxiv.org/html/2406.18403v1]\n",
    "\n",
    "## üí° Strategy\n",
    "\n",
    "To attempt at doing this, we will use LLMs to understand relationships with the prompt, context and answer pairing and have it deduce a relationship. Typical approach starts with Adding your Data, then into Prompt Engineering, finally with Fine-tuning. In extreme circumstances would pre-training ever be considered, which we will rule out as an option here.\n",
    "\n",
    "We will demonstrate how to fine-tune using the model, and acknowledge that there are additional strategies that we will not go to. These do include:\n",
    "\n",
    "1. RLHF, which will allow for reinforcement learning from human feedback to further improve the model based on input as the model progresses over time\n",
    "2. Chain-of-Thought evaluation, citing the results of human-correlation performance with the G-Eval paper: (https://arxiv.org/abs/2303.16634)[https://arxiv.org/abs/2303.16634]\n",
    "\n",
    "## üß† Data Inputs\n",
    "\n",
    "The inputs we would use as part of all our will follow the similar syntax:\n",
    "\n",
    "* Question - this is the question being asked on behalf of our QnA system\n",
    "* Ground Truth - this is the actual answer the bot should supply\n",
    "* Prompt Answer - this is the answer our QnA system would provide\n",
    "* Human Evaluation - this is the output our evaluator would provide\n",
    "* Generalized Context - this is general context provided to the prompt, perhaps on a specific topic retrieved by the question.\n",
    "* Specialized Context - this is specialized information that can give greater context specific to the question being asked\n",
    "\n",
    "The above structure can be formatted as you see fit for your use case. The below information is simply used as an example, nothing more.\n",
    "\n",
    "Consider this as an example for your data set.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"question\": \"Are eye check-ups included in my plan?\",\n",
    "    \"truth\": \"Some additional services provided by the health plan include: INN: $0.00 copay per visit. (Includes check-up and assessment). Limited to one visit per year. OON: Not included\",\n",
    "    \"answer\": \"Eye exams are covered in-network with a $0.00 copay, limited to one annual visit. Out-of-network services are not included.\",\n",
    "    \"evaluation\": \"5: Excellent response\",\n",
    "    \"general_context\": \"Eye exams are generally not included under standard healthcare plans. However, they may be part of supplemental benefits under certain health plans, possibly managed by a third-party provider.\",\n",
    "    \"specialized_context\": \"Eye Examination (with Assessment) - In Network: - $0.00 copay per visit (Includes check-up and assessment) - Limited to one visit per year - Copay/Coinsurance does not contribute toward the In-Network Maximum - Out of Network: - Not covered. Review the plan details for further information.\"\n",
    "}\n",
    "```\n",
    "\n",
    "## üõ†Ô∏è Prerequities\n",
    "\n",
    "To move forward, we need to setup and have the following available:\n",
    "\n",
    "### Infrastructure\n",
    "\n",
    "* An Azure Subscription\n",
    "* Access to Azure OpenAI Service, in a [region](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#fine-tuning-models) that supports fine-tuning\n",
    "\n",
    "### Data\n",
    "\n",
    "* Access to prepared datasets for both training and validations:\n",
    "    * Preferably 50 high-quality samples, 1000s is even better.\n",
    "\n",
    "### Software\n",
    "\n",
    "* Python 3.7.1 or greater available\n",
    "* Python libraries installed, consistent with `requirements.txt`\n",
    "* Jupyter Notebook runtime available to run\n",
    "\n",
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "1. To run this notebook, you will need to setup the `.env` consistent with the `.env.sample`. Populate this with the Azure OpenAI key and endpoint.\n",
    "2. Install the pip requirements made available, we would recommend a `virtualenv` which you can install into: `pip install -r requirements.txt`\n",
    "    1. For conda users, you can use: `conda create --name custom-human-evaluator python=3.9 -y`\n",
    "    2. `conda activate custom-human-evaluator`\n",
    "3. Run the following cells to run the examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d703e",
   "metadata": {},
   "source": [
    "### Step 1: Python Dependencies and Setup\n",
    "\n",
    "#### Import required Python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebe593",
   "metadata": {},
   "source": [
    "#### Load Azure OpenAI credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b7a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"AZURE_AOAI_DEPLOYMENT_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_AOAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_AOAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdf556",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Training & Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f50c13",
   "metadata": {},
   "source": [
    "### CSV to JSON convert, with field mappings!\n",
    "\n",
    "Feel free to use the following code to help stage the file for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee0674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import csv_to_jsonl_with_mapping\n",
    "\n",
    "# Example usage - moves up 1 directories to traverse to Data folder\n",
    "source_csv = \"%s/my_utils/data/evaluations/jsonl/trainingdata.csv\" % os.getcwd()\n",
    "output_jsonl = \"%s/my_utils/data/evaluations/jsonl/CustomHumanEvaluator-Customer.jsonl\" % os.getcwd()\n",
    "\n",
    "# Define the key mapping from input keys to output keys\n",
    "key_mapping = {\n",
    "    \"Question\": \"question\",\n",
    "    \"Ground Truth\": \"truth\",\n",
    "    \"Blended Response - Prompt 1\": \"answer\",\n",
    "    \"Rating 1-3-5 (P1)\": \"evaluation\",\n",
    "    \"General Documents Summary\": \"general_context\",\n",
    "    \"Specialized  Benefit information summary\": \"specialized_context\"\n",
    "}\n",
    "\n",
    "# Call the function with the source CSV, output path, and key mapping\n",
    "csv_to_jsonl_with_mapping(source_csv, output_jsonl, key_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bed57b",
   "metadata": {},
   "source": [
    "#### Generate the training and validation test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115ee01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Dataset Stats ---\n",
      "5: 5 instances\n",
      "3: 2 instances\n",
      "1: 4 instances\n",
      "Total: 11\n",
      "\n",
      "--- Incremental Training Dataset Stats ---\n",
      "5: 3 instances\n",
      "Total: 3\n",
      "\n",
      "--- Validation Dataset Stats ---\n",
      "5: 2 instances\n",
      "1: 2 instances\n",
      "3: 1 instances\n",
      "Total: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.utils import split_dataset\n",
    "\n",
    "# Example usage\n",
    "# output_jsonl = 'path_to_your_jsonl_file.jsonl'  # Uncomment with your file path if not using above.\n",
    "field_to_optimize = 'evaluation'  # The field on which to optimize the distribution\n",
    "train_data, incr_data, val_data = split_dataset(output_jsonl, field_to_optimize, training_split=0.6, incremental_split=0.2, random_seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6f300",
   "metadata": {},
   "source": [
    "### Step 2: We will try a n-shot learning approach for the model so that it can best predict the output. Here we will use a combination of our incremental training and validation set as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afecf838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data!\n",
      "Processing data!\n",
      "Processing data!\n",
      "Processing data!\n",
      "Processing data!\n"
     ]
    }
   ],
   "source": [
    "from src.utils import generate_nshot_prompt\n",
    "import re\n",
    "\n",
    "prompt = \"You are a evaluator on behalf of providing scores of responses by call-center agents. Your answer should only be one number between 0 and 5. Your goal is to match the behavior of the human evaluators as closely as possible. \\n\\n\"\n",
    "evaluations = []\n",
    "for data in val_data:\n",
    "    print(\"Processing data!\")\n",
    "    # Generate the many-shot prompt\n",
    "    prompt = generate_nshot_prompt(prompt, train_data, data, n=9)\n",
    "\n",
    "    # Call Azure OpenAI to evaluate the new example\n",
    "    completion = client.chat.completions.create(  # This is the updated API method\n",
    "        model=\"gpt-4o\",  # Update to the engine you're using, such as \"gpt-35-turbo\" or \"gpt-4\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a healthcare call center advocate, who is evaluating answers based on answer completeness and accuracy.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,  # Adjust as necessary\n",
    "        temperature=0.7  # Controls creativity, adjust based on your requirements\n",
    "    )\n",
    "\n",
    "    # Print the evaluation from Azure OpenAI\n",
    "    llmEvaluation = completion.choices[0].message.content.strip()\n",
    "    evaluations.append((data, int(llmEvaluation), data['evaluation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195fb65",
   "metadata": {},
   "source": [
    "We're able to calculate the accuracy based on our data so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a61565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum([1 for data, llmEval, humanEvaluation in evaluations if llmEval == humanEvaluation]) / len(evaluations)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8205cf",
   "metadata": {},
   "source": [
    "This approach is a simple way, but it does not provide near-perfect correlation with human evaluations. Additionally, we quickly run out of context size so this approach would not scale well to numerous examples when we want to train on data of examples in the thousands.\n",
    "\n",
    "To achieve better results, you can use a more advanced approach, such as fine-tuning the model on your data. This requires more data and computational resources, but it can significantly improve the performance of the model. Let's try this next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c83d3",
   "metadata": {},
   "source": [
    "### Step 3: Upload Datasets for Fine-Tuning\n",
    "\n",
    "We want fine-tuning to detect the behaviors in our training data set, that is it takes the full context, paired with input data, and maps it to a targeted human evaluation response. Fine-tuning on Azure OpenAI uses the LoRA algorithm to create an additional vector space of embeddings that closely resemble the behaviors the model will begin to learn. The more data points we throw at it, the better the model can be at detecting patterns and representations of question and answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f54a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import convert_to_jsonl_in_memory, generate_prompt\n",
    "import io\n",
    "from io import BufferedReader\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "training_file_name = \"training_set.jsonl\"\n",
    "validation_file_name = \"validation_set.jsonl\"\n",
    "\n",
    "finetuning_train_data = []\n",
    "for data in train_data:\n",
    "    dataDict = {}\n",
    "    dataDict[\"messages\"] = []\n",
    "    dataDict[\"messages\"].append({\"role\": \"system\", \"content\": \"You are a healthcare call center agent, who is evaluating answers based on answer completeness and accuracy.\"})\n",
    "    dataDict[\"messages\"].append({\"role\": \"user\", \"content\": generate_prompt(data)})\n",
    "    dataDict[\"messages\"].append({\"role\": \"assistant\", \"content\": data[\"evaluation\"]})\n",
    "    finetuning_train_data.append(dataDict)\n",
    "\n",
    "finetuning_val_data = []\n",
    "for data in val_data:\n",
    "    dataDict = {}\n",
    "    dataDict[\"messages\"] = []\n",
    "    dataDict[\"messages\"].append({\"role\": \"system\", \"content\": \"You are a healthcare call center agent, who is evaluating answers based on answer completeness and accuracy.\"})\n",
    "    dataDict[\"messages\"].append({\"role\": \"user\", \"content\": generate_prompt(data)})\n",
    "    dataDict[\"messages\"].append({\"role\": \"assistant\", \"content\": data[\"evaluation\"]})\n",
    "    finetuning_val_data.append(dataDict)\n",
    "\n",
    "training_jsonl_content = convert_to_jsonl_in_memory(finetuning_train_data)\n",
    "training_jsonl_bytes = io.BytesIO(training_jsonl_content.encode('utf-8'))\n",
    "\n",
    "validation_jsonl_content = convert_to_jsonl_in_memory(finetuning_val_data)\n",
    "validation_jsonl_bytes = io.BytesIO(validation_jsonl_content.encode('utf-8'))\n",
    "\n",
    "training_response = client.files.create(\n",
    "    file=(training_file_name, training_jsonl_bytes), \n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file=(validation_file_name, validation_jsonl_bytes), \n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559f09f",
   "metadata": {},
   "source": [
    "We must first confirm that the file status has completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3b11d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file status: processed\n",
      "Training file status: processed\n"
     ]
    }
   ],
   "source": [
    "# Retrieve file status\n",
    "while True:\n",
    "    training_file_status = client.files.retrieve(training_file_id)\n",
    "    print(f\"Training file status: {training_file_status.status}\")\n",
    "\n",
    "    validation_file_status = client.files.retrieve(validation_file_id)\n",
    "    print(f\"Training file status: {validation_file_status.status}\")\n",
    "    if (training_file_status.status != \"pending\" and validation_file_status.status != \"pending\") and \\\n",
    "        (training_file_status.status != \"running\" and validation_file_status.status != \"running\"):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee27a",
   "metadata": {},
   "source": [
    "### Step 4: Begin Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927f0c4",
   "metadata": {},
   "source": [
    "Now you can submit your fine-tuning training job. \n",
    "\n",
    "The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "You can use the job ID to monitor the status of the fine-tuning job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e925985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-dcf10753567b43d0aa67af4bcf42b59b\n",
      "Status: pending\n",
      "FineTuningJob(id='ftjob-dcf10753567b43d0aa67af4bcf42b59b', created_at=1725588833, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=-1, batch_size=-1, learning_rate_multiplier=1), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id=None, result_files=None, seed=None, status='pending', trained_tokens=None, training_file='file-d53050ccdcf548d793854354a31f5c69', validation_file='file-2ba3a48947584c49b3aff3404e018b07', estimated_finish=None, integrations=None, updated_at=1725588833)\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-4o-2024-08-06\", # Enter base model name. Note that in Azure OpenAI the model name contains dashes and cannot contain dot/period characters. \n",
    "    seed = 105  # seed parameter controls reproducibility of the fine-tuning job. If no seed is specified one will be generated automatically.\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa608e2f",
   "metadata": {},
   "source": [
    "### Step 5: Track Fine-Tuning Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5ad52",
   "metadata": {},
   "source": [
    "You can track the training job status by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa4b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job ftjob-dcf10753567b43d0aa67af4bcf42b59b finished with status: succeeded\n",
      "Checking other fine-tune jobs for this resource.\n",
      "Found 2 fine-tune jobs.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Track fine-tuning job training status\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the status of our fine-tuning job.\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "status = response.status\n",
    "\n",
    "# If the job isn't done yet, poll it every 10 seconds.\n",
    "while status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(10)\n",
    "    \n",
    "    response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    print(response)\n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "    status = response.status\n",
    "    print(f\"Status: {status}\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"Fine-tuning job {job_id} finished with status: {status}\")\n",
    "\n",
    "# List all fine-tuning jobs for this resource.\n",
    "print(\"Checking other fine-tune jobs for this resource.\")\n",
    "response = client.fine_tuning.jobs.list()\n",
    "print(f'Found {len(response.data)} fine-tune jobs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afeb619",
   "metadata": {},
   "source": [
    "To get the full results, you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09f1d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-dcf10753567b43d0aa67af4bcf42b59b', created_at=1725588833, error=None, fine_tuned_model='gpt-4o-2024-08-06.ft-dcf10753567b43d0aa67af4bcf42b59b', finished_at=1725592251, hyperparameters=Hyperparameters(n_epochs=9, batch_size=1, learning_rate_multiplier=1), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id=None, result_files=['file-9faaf6d1afba4814a4500a4ddd4ebdaf'], seed=None, status='succeeded', trained_tokens=30447, training_file='file-d53050ccdcf548d793854354a31f5c69', validation_file='file-2ba3a48947584c49b3aff3404e018b07', estimated_finish=None, integrations=None, updated_at=1725592251)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve fine_tuned_model name\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(response)\n",
    "\n",
    "fine_tuned_model = response.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a58b85",
   "metadata": {},
   "source": [
    "### Step 6: Deploy The Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370097d4",
   "metadata": {},
   "source": [
    "Model deployment must be done using the [REST API](https://learn.microsoft.com/en-us/rest/api/cognitiveservices/accountmanagement/deployments/create-or-update?view=rest-cognitiveservices-accountmanagement-2023-05-01&tabs=HTTP), which requires separate authorization, a different API path, and a different API version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53296c51",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>variable</th>\n",
    "<th>Definition</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>token</td>\n",
    "<td>There are multiple ways to generate an authorization token. The easiest method for initial testing is to launch the Cloud Shell from the <a href=\"https://portal.azure.com\" data-linktype=\"external\">Azure portal</a>. Then run <a href=\"/en-us/cli/azure/account#az-account-get-access-token()\" data-linktype=\"absolute-path\"><code>az account get-access-token</code></a>. You can use this token as your temporary authorization token for API testing. We recommend storing this in a new environment variable</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>subscription</td>\n",
    "<td>The subscription ID for the associated Azure OpenAI resource</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>resource_group</td>\n",
    "<td>The resource group name for your Azure OpenAI resource</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>resource_name</td>\n",
    "<td>The Azure OpenAI resource name</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>model_deployment_name</td>\n",
    "<td>The custom name for your new fine-tuned model deployment. This is the name that will be referenced in your code when making chat completion calls.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>fine_tuned_model</td>\n",
    "<td>Retrieve this value from your fine-tuning job results in the previous step. It will look like <code>gpt-35-turbo-0613.ft-b044a9d3cf9c4228b5d393567f693b83</code>. You will need to add that value to the deploy_data json.</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38e895",
   "metadata": {},
   "source": [
    "Make sure you have your `az login` command setup and authenticated, otherwise the below will fail! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3848e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new deployment...\n",
      "<Response [201]>\n",
      "Created\n",
      "{'id': '/subscriptions/28d2df62-e322-4b25-b581-c43b94bd2607/resourceGroups/uhg-advassist-slm-eval/providers/Microsoft.CognitiveServices/accounts/slm-human-eval/deployments/custom-evaluator-model', 'type': 'Microsoft.CognitiveServices/accounts/deployments', 'name': 'custom-evaluator-model', 'sku': {'name': 'standard', 'capacity': 1}, 'properties': {'model': {'format': 'OpenAI', 'name': 'gpt-4o-2024-08-06.ft-dcf10753567b43d0aa67af4bcf42b59b', 'version': '1'}, 'versionUpgradeOption': 'NoAutoUpgrade', 'capabilities': {'chatCompletion': 'true', 'maxContextToken': '128000', 'maxOutputToken': '16384'}, 'provisioningState': 'Creating'}, 'systemData': {'createdBy': 'marcjimenez@microsoft.com', 'createdByType': 'User', 'createdAt': '2024-09-06T19:25:44.7487275Z', 'lastModifiedBy': 'marcjimenez@microsoft.com', 'lastModifiedByType': 'User', 'lastModifiedAt': '2024-09-06T19:25:44.7487275Z'}, 'etag': '\"e87a8003-6555-48f6-a41f-28ce886593c6\"'}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# get az token\n",
    "result = subprocess.run([\"az\", \"account\", \"get-access-token\"], capture_output=True, text=True, check=True)\n",
    "token = json.loads(result.stdout)[\"accessToken\"]\n",
    "\n",
    "# Parse the JSON output into a Python dictionary\n",
    "output_dict = json.loads(result.stdout)\n",
    "\n",
    "subscription = os.getenv(\"AZURE_AI_STUDIO_SUBSCRIPTION_ID\") \n",
    "resource_group = os.getenv(\"AZURE_AI_STUDIO_RESOURCE_GROUP_NAME\")\n",
    "resource_name = os.getenv(\"AZURE_AOAI_ENDPOINT\").split('/')[-2].split('.')[0] #returns resource name from the AOIA endpoint\n",
    "model_deployment_name =\"custom-evaluator-model\" \n",
    "\n",
    "deploy_params = {\"api-version\": \"2023-05-01\"} \n",
    "deploy_headers = {\"Authorization\": \"Bearer {}\".format(token), \"Content-Type\": \"application/json\"}\n",
    "deploy_data = {\n",
    "    \"sku\": {\"name\": \"standard\", \"capacity\": 1}, \n",
    "    \"properties\": {\n",
    "        \"model\": {\n",
    "            \"format\": \"OpenAI\",\n",
    "            \"name\": fine_tuned_model, #retrieve this value from the previous call, it will look like gpt-35-turbo-0613.ft-b044a9d3cf9c4228b5d393567f693b83\n",
    "            \"version\": \"1\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "deploy_data = json.dumps(deploy_data)\n",
    "\n",
    "print(\"Creating a new deployment...\")\n",
    "request_url = f\"https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{model_deployment_name}\"\n",
    "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
    "\n",
    "print(r)\n",
    "print(r.reason)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1259251",
   "metadata": {},
   "source": [
    "This will take quite a bit of time to run, be sure to keep an eye on it and clean up resources as soon as you're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe6e0b",
   "metadata": {},
   "source": [
    "### Step 7: Test And Use The Deployed Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e5bbc",
   "metadata": {},
   "source": [
    "After your fine-tuned model is deployed, you can use it like any other deployed model in either the [Chat Playground of Azure OpenAI Studio](https://oai.azure.com/), or via the chat completion API. \n",
    "\n",
    "For example, you can send a chat completion call to your deployed model, as shown in the following Python code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for data in finetuning_val_data:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_deployment_name, # engine = \"Custom deployment name you chose for your fine-tuning model\"\n",
    "        messages=data[\"messages\"][:-1], # Remove the last message which is the ground truth\n",
    "    )\n",
    "    print(\"Expected %s, got: %s\", (data[\"messages\"][2]['content'], response.choices[0].message.content))\n",
    "\n",
    "# print(response)\n",
    "# print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65563bf0",
   "metadata": {},
   "source": [
    "### Step 8: Delete The Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb666e8",
   "metadata": {},
   "source": [
    "It is **strongly recommended** that once you're done with this tutorial and have tested a few chat completion calls against your fine-tuned model, that you delete the model deployment, since the fine-tuned / customized models have an [hourly hosting cost](https://azure.microsoft.com/zh-cn/pricing/details/cognitive-services/openai-service/#pricing) associated with them once they are deployed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
